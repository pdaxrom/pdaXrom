---
 include/linux/irq.h   |   10 ++++------
 kernel/irq/handle.c   |   13 +++++++++++--
 kernel/irq/manage.c   |   22 ++++++++++++++++------
 kernel/irq/spurious.c |    3 +--
 4 files changed, 32 insertions(+), 16 deletions(-)

Index: linux-2.6.23.1-rt5/include/linux/irq.h
===================================================================
--- linux-2.6.23.1-rt5.orig/include/linux/irq.h
+++ linux-2.6.23.1-rt5/include/linux/irq.h
@@ -146,7 +146,6 @@ struct irq_chip {
  * @last_unhandled:	aging timer for unhandled count
  * @thread:		Thread pointer for threaded preemptible irq handling
  * @wait_for_handler:	Waitqueue to wait for a running preemptible handler
- * @cycles:		Timestamp for stats and debugging
  * @lock:		locking for SMP
  * @affinity:		IRQ affinity on SMP
  * @cpu:		cpu index useful for balancing
@@ -169,10 +168,10 @@ struct irq_desc {
 	unsigned int		irq_count;	/* For detecting broken IRQs */
 	unsigned int		irqs_unhandled;
 	unsigned long		last_unhandled;	/* Aging timer for unhandled count */
- 	struct task_struct	*thread;
- 	wait_queue_head_t	wait_for_handler;
- 	cycles_t		timestamp;
-	spinlock_t		lock;
+	struct task_struct	*thread;
+	wait_queue_head_t	wait_for_handler;
+	cycles_t		timestamp;
+	raw_spinlock_t		lock;
 #ifdef CONFIG_SMP
 	cpumask_t		affinity;
 	unsigned int		cpu;
@@ -398,7 +397,6 @@ extern int set_irq_msi(unsigned int irq,
 
 /* Early initialization of irqs */
 extern void early_init_hardirqs(void);
-extern cycles_t irq_timestamp(unsigned int irq);
 
 #if defined(CONFIG_PREEMPT_HARDIRQS)
 extern void init_hardirqs(void);
Index: linux-2.6.23.1-rt5/kernel/irq/handle.c
===================================================================
--- linux-2.6.23.1-rt5.orig/kernel/irq/handle.c
+++ linux-2.6.23.1-rt5/kernel/irq/handle.c
@@ -54,12 +54,13 @@ struct irq_desc irq_desc[NR_IRQS] __cach
 		.chip = &no_irq_chip,
 		.handle_irq = handle_bad_irq,
 		.depth = 1,
-		.lock = __SPIN_LOCK_UNLOCKED(irq_desc->lock),
+		.lock = RAW_SPIN_LOCK_UNLOCKED(irq_desc),
 #ifdef CONFIG_SMP
 		.affinity = CPU_MASK_ALL
 #endif
 	}
 };
+EXPORT_SYMBOL_GPL(irq_desc);
 
 /*
  * What should we do if we get a hw irq event on an illegal vector?
@@ -151,6 +152,7 @@ irqreturn_t handle_IRQ_event(unsigned in
 
 		ret = action->handler(irq, action->dev_id);
 		if (preempt_count() != preempt_count) {
+			stop_trace();
 			print_symbol("BUG: unbalanced irq-handler preempt count in %s!\n", (unsigned long) action->handler);
 			printk("entered with %08x, exited with %08x.\n", preempt_count, preempt_count());
 			dump_stack();
@@ -225,7 +227,7 @@ int redirect_hardirq(struct irq_desc *de
  * This is the original x86 implementation which is used for every
  * interrupt type.
  */
-fastcall unsigned int __do_IRQ(unsigned int irq)
+fastcall notrace unsigned int __do_IRQ(unsigned int irq)
 {
 	struct irq_desc *desc = irq_desc + irq;
 	struct irqaction *action;
@@ -246,6 +248,13 @@ fastcall unsigned int __do_IRQ(unsigned 
 		desc->chip->end(irq);
 		return 1;
 	}
+	/*
+	 * If the task is currently running in user mode, don't
+	 * detect soft lockups.  If CONFIG_DETECT_SOFTLOCKUP is not
+	 * configured, this should be optimized out.
+	 */
+	if (user_mode(get_irq_regs()))
+		touch_softlockup_watchdog();
 
 	spin_lock(&desc->lock);
 	if (desc->chip->ack)
Index: linux-2.6.23.1-rt5/kernel/irq/manage.c
===================================================================
--- linux-2.6.23.1-rt5.orig/kernel/irq/manage.c
+++ linux-2.6.23.1-rt5/kernel/irq/manage.c
@@ -492,9 +492,9 @@ void free_irq(unsigned int irq, void *de
 		 * We do this after actually deregistering it, to make sure that
 		 * a 'real' IRQ doesn't run in parallel with our fake
 		 */
-		local_irq_save(flags);
+		local_irq_save_nort(flags);
 		handler(irq, dev_id);
-		local_irq_restore(flags);
+		local_irq_restore_nort(flags);
 	}
 #endif
 }
@@ -579,9 +579,9 @@ int request_irq(unsigned int irq, irq_ha
 		 */
 		unsigned long flags;
 
-		local_irq_save(flags);
+		local_irq_save_nort(flags);
 		handler(irq, dev_id);
-		local_irq_restore(flags);
+		local_irq_restore_nort(flags);
 	}
 #endif
 
@@ -599,6 +599,11 @@ int hardirq_preemption = 1;
 
 EXPORT_SYMBOL(hardirq_preemption);
 
+/*
+ * Real-Time Preemption depends on hardirq threading:
+ */
+#ifndef CONFIG_PREEMPT_RT
+
 static int __init hardirq_preempt_setup (char *str)
 {
 	if (!strncmp(str, "off", 3))
@@ -613,6 +618,7 @@ static int __init hardirq_preempt_setup 
 
 __setup("hardirq-preempt=", hardirq_preempt_setup);
 
+#endif
 
 /*
  * threaded simple handler
@@ -772,12 +778,16 @@ static int do_irqd(void * __desc)
 	sys_sched_setscheduler(current->pid, SCHED_FIFO, &param);
 
 	while (!kthread_should_stop()) {
-		local_irq_disable();
+		local_irq_disable_nort();
 		set_current_state(TASK_INTERRUPTIBLE);
+#ifndef CONFIG_PREEMPT_RT
 		irq_enter();
+#endif
 		do_hardirq(desc);
+#ifndef CONFIG_PREEMPT_RT
 		irq_exit();
-		local_irq_enable();
+#endif
+		local_irq_enable_nort();
 		cond_resched();
 #ifdef CONFIG_SMP
 		/*
Index: linux-2.6.23.1-rt5/kernel/irq/spurious.c
===================================================================
--- linux-2.6.23.1-rt5.orig/kernel/irq/spurious.c
+++ linux-2.6.23.1-rt5/kernel/irq/spurious.c
@@ -59,9 +59,8 @@ static int misrouted_irq(int irq)
 			}
 			action = action->next;
 		}
-		local_irq_disable();
 		/* Now clean up the flags */
-		spin_lock(&desc->lock);
+		spin_lock_irq(&desc->lock);
 		action = desc->action;
 
 		/*
