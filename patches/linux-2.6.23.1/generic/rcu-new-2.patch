From paulmck@linux.vnet.ibm.com Thu Sep 27 00:03:00 2007
Date: Mon, 10 Sep 2007 11:33:05 -0700
From: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
To: linux-kernel@vger.kernel.org
Cc: linux-rt-users@vger.kernel.org, mingo@elte.hu, akpm@linux-foundation.org,
     dipankar@in.ibm.com, josht@linux.vnet.ibm.com, tytso@us.ibm.com,
     dvhltc@us.ibm.com, tglx@linutronix.de, a.p.zijlstra@chello.nl,
     bunk@kernel.org, ego@in.ibm.com, oleg@tv-sign.ru, srostedt@redhat.com
Subject: [PATCH RFC 2/9] RCU: Fix barriers

Work in progress, not for inclusion.

Fix rcu_barrier() to work properly in preemptive kernel environment.
Also, the ordering of callback must be preserved while moving
callbacks to another CPU during CPU hotplug.

Signed-off-by: Dipankar Sarma <dipankar@in.ibm.com>
Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
---

 kernel/rcuclassic.c |    2 +-
 kernel/rcupdate.c   |   10 ++++++++++
 2 files changed, 11 insertions(+), 1 deletion(-)

Index: linux-2.6.23.1-rt5/kernel/rcuclassic.c
===================================================================
--- linux-2.6.23.1-rt5.orig/kernel/rcuclassic.c
+++ linux-2.6.23.1-rt5/kernel/rcuclassic.c
@@ -353,9 +353,9 @@ static void __rcu_offline_cpu(struct rcu
 	if (rcp->cur != rcp->completed)
 		cpu_quiet(rdp->cpu, rcp);
 	spin_unlock_bh(&rcp->lock);
+	rcu_move_batch(this_rdp, rdp->donelist, rdp->donetail);
 	rcu_move_batch(this_rdp, rdp->curlist, rdp->curtail);
 	rcu_move_batch(this_rdp, rdp->nxtlist, rdp->nxttail);
-	rcu_move_batch(this_rdp, rdp->donelist, rdp->donetail);
 }
 
 static void rcu_offline_cpu(int cpu)
Index: linux-2.6.23.1-rt5/kernel/rcupdate.c
===================================================================
--- linux-2.6.23.1-rt5.orig/kernel/rcupdate.c
+++ linux-2.6.23.1-rt5/kernel/rcupdate.c
@@ -117,7 +117,17 @@ void rcu_barrier(void)
 	mutex_lock(&rcu_barrier_mutex);
 	init_completion(&rcu_barrier_completion);
 	atomic_set(&rcu_barrier_cpu_count, 0);
+	/*
+	 * The queueing of callbacks in all CPUs must be atomic with
+	 * respect to RCU, otherwise one CPU may queue a callback,
+	 * wait for a grace period, decrement barrier count and call
+	 * complete(), while other CPUs have not yet queued anything.
+	 * So, we need to make sure that grace periods cannot complete
+	 * until all the callbacks are queued.
+	 */
+	rcu_read_lock();
 	on_each_cpu(rcu_barrier_func, NULL, 0, 1);
+	rcu_read_unlock();
 	wait_for_completion(&rcu_barrier_completion);
 	mutex_unlock(&rcu_barrier_mutex);
 }
