This patch adds remote function calls to control the preformance
monitor on the remote CPU which the target SPU program runs.

This change is necessary for spe-follow on SMP platforms.
because the CPU which SPU events are notified may be not 
same as the CPU which the target SPU program runs.

Signed-off-by: Takashi Yamamoto <TakashiA.Yamamoto@jp.sony.com>
---
 arch/powerpc/perfmon/perfmon.c      |    5 
 arch/powerpc/perfmon/perfmon_cell.c |  448 ++++++++++++++++++++++++++++++++++--
 2 files changed, 432 insertions(+), 21 deletions(-)

--- a/arch/powerpc/perfmon/perfmon.c
+++ b/arch/powerpc/perfmon/perfmon.c
@@ -58,6 +58,7 @@ static void pfm_stop_active(struct task_
 int pfm_arch_ctxswout_thread(struct task_struct *task, struct pfm_context *ctx)
 {
 	struct pfm_arch_pmu_info *arch_info;
+	int need_save_pmds = 1;
 
 	arch_info = pfm_pmu_info();
 	/*
@@ -71,9 +72,9 @@ int pfm_arch_ctxswout_thread(struct task
 	pfm_stop_active(task, ctx, ctx->active_set);
 
 	if (arch_info->ctxswout_thread)
-		arch_info->ctxswout_thread(task, ctx, ctx->active_set);
+		need_save_pmds = arch_info->ctxswout_thread(task, ctx, ctx->active_set);
 
-	return pfm_arch_is_active(ctx);
+	return (pfm_arch_is_active(ctx) && need_save_pmds);
 }
 
 /*
--- a/arch/powerpc/perfmon/perfmon_cell.c
+++ b/arch/powerpc/perfmon/perfmon_cell.c
@@ -57,6 +57,18 @@ struct pfm_cell_platform_pmu_info {
 	int (*rtas_call)(int token, int param1, int param2, int *param3, ...);
 };
 
+/**
+ * pfm_cell_remote_req
+ **/
+struct pfm_cell_remote_req {
+	struct task_struct   *task;
+	struct pfm_context   *ctx;
+	struct pfm_event_set *set;
+	int                  result;
+	unsigned int         cnum;
+	u64                  pmd;
+};
+
 /*
  * Mapping from Perfmon logical control registers to Cell hardware registers.
  */
@@ -667,6 +679,22 @@ static void pfm_cell_write_pmc(struct pf
 }
 
 /**
+ * get_target_cpu
+ *
+ */
+static int get_target_cpu(struct pfm_context *ctx)
+{
+	struct pfm_arch_context *ctx_arch = pfm_ctx_arch(ctx);
+	struct spu *spu = ctx_arch->arg;
+	int cpu = smp_processor_id();
+
+	if (spu)
+		cpu = ((cpu & 0x1) | (spu->node << 1));
+
+	return cpu;
+}
+
+/**
  * pfm_cell_write_pmd
  **/
 static void pfm_cell_write_pmd(struct pfm_context *ctx,
@@ -682,6 +710,51 @@ static void pfm_cell_write_pmd(struct pf
 }
 
 /**
+ * pfm_cell_write_pmd_remote
+ *
+ * This function is the wrapper of pfm_cell_write_pmd to
+ * write pmd on the remote CPU(node).
+ **/
+static void pfm_cell_write_pmd_remote(void *arg)
+{
+	struct pfm_cell_remote_req *req = arg;
+
+	pfm_cell_write_pmd(req->ctx, req->cnum, req->pmd);
+	return ;
+}
+
+/**
+ * pfm_cell_write_pmd_smp
+ **/
+static void pfm_cell_write_pmd_smp(struct pfm_context *ctx,
+				   unsigned int cnum, u64 value)
+{
+	int target_cpu;
+	struct pfm_cell_remote_req req;
+
+	if (ctx->flags.not_dflt_ctxsw && !ctx->flags.system) {
+		target_cpu = get_target_cpu(ctx);
+		if (target_cpu == smp_processor_id()) {
+			pfm_cell_write_pmd(ctx, cnum, value);
+			return ;
+		}
+
+		if (irqs_disabled()) {
+			PFM_DBG("skip write pmd");
+			return;
+		}
+		req.ctx = ctx;
+		req.cnum = cnum;
+		req.pmd = value;
+		smp_call_function_single(target_cpu,
+					 pfm_cell_write_pmd_remote,
+					 &req, 0, 1);
+	} else {
+		pfm_cell_write_pmd(ctx, cnum, value);
+	}
+}
+
+/**
  * pfm_cell_read_pmd
  **/
 static u64 pfm_cell_read_pmd(struct pfm_context *ctx,
@@ -691,14 +764,61 @@ static u64 pfm_cell_read_pmd(struct pfm_
 	struct pfm_cell_platform_pmu_info *info =
 		((struct pfm_arch_pmu_info *)
 		 (pfm_pmu_conf->pmu_info))->platform_info;
+	u64 ret;
 
-	if (cnum < NR_CTRS)
-		return info->read_ctr(cpu, cnum);
+	if (cnum < NR_CTRS) {
+		ret = info->read_ctr(cpu, cnum);
+		PFM_DBG("read pmd. cntr:%d data:0x%lx", cnum, ret);
+		return ret;
+	}
 
 	return -EINVAL;
 }
 
 /**
+ * pfm_cell_read_pmd_remote
+ *
+ * This function is the wrapper of pfm_cell_read_pmd to
+ * read pmd on the remote CPU(node).
+ **/
+static void pfm_cell_read_pmd_remote(void *arg)
+{
+	struct pfm_cell_remote_req *req = arg;
+
+	req->pmd = pfm_cell_read_pmd(req->ctx, req->cnum);
+	return ;
+}
+
+/**
+ * pfm_cell_read_pmd_smp
+ **/
+static u64 pfm_cell_read_pmd_smp(struct pfm_context *ctx, unsigned int cnum)
+{
+	int target_cpu;
+	struct pfm_cell_remote_req req;
+
+	if (ctx->flags.not_dflt_ctxsw && !ctx->flags.system) {
+		target_cpu = get_target_cpu(ctx);
+		if (target_cpu == smp_processor_id())
+			return pfm_cell_read_pmd(ctx, cnum);
+
+		if (irqs_disabled()) {
+			PFM_DBG("skip read pmd");
+			return 0;
+		}
+		req.ctx = ctx;
+		req.cnum = cnum;
+		req.pmd = 0;
+		smp_call_function_single(target_cpu,
+					 pfm_cell_read_pmd_remote,
+					 &req, 0, 1);
+		return req.pmd;
+	} else {
+		return pfm_cell_read_pmd(ctx, cnum);
+	}
+}
+
+/**
  * pfm_cell_enable_counters
  *
  * Just need to turn on the global disable bit in pm_control.
@@ -717,6 +837,48 @@ static void pfm_cell_enable_counters(str
 }
 
 /**
+ * pfm_cell_enable_counters_remote
+ *
+ * This function is the wrapper of pfm_cell_enable_counters
+ * to enable counters on the remote CPU(node).
+ **/
+static void pfm_cell_enable_counters_remote(void *arg)
+{
+	struct pfm_cell_remote_req *req = arg;
+	pfm_cell_enable_counters(req->ctx, req->set);
+}
+
+/**
+ * pfm_cell_enable_counters_smp
+ *
+ **/
+static void pfm_cell_enable_counters_smp(struct pfm_context *ctx,
+					 struct pfm_event_set *set)
+{
+	int target_cpu;
+	struct pfm_cell_remote_req req;
+
+	if (ctx->flags.not_dflt_ctxsw && !ctx->flags.system) {
+		target_cpu = get_target_cpu(ctx);
+		if (target_cpu == smp_processor_id()) {
+			pfm_cell_enable_counters(ctx, set);
+			return ;
+		}
+
+		if (irqs_disabled())
+			return ;
+
+		req.ctx = ctx;
+		req.set = set;
+		smp_call_function_single(target_cpu,
+					 pfm_cell_enable_counters_remote,
+					 &req, 0, 1);
+	} else {
+		pfm_cell_enable_counters(ctx, set);
+	}
+}
+
+/**
  * pfm_cell_disable_counters
  *
  * Just need to turn off the global disable bit in pm_control.
@@ -736,6 +898,48 @@ static void pfm_cell_disable_counters(st
 	reset_signals(smp_processor_id());
 }
 
+/**
+ * pfm_cell_disable_counters_remote
+ *
+ * This function is the wrapper of pfm_cell_disable_counters
+ * to disable counters on the remote CPU(node).
+ **/
+static void pfm_cell_disable_counters_remote(void *arg)
+{
+	struct pfm_cell_remote_req *req = arg;
+	pfm_cell_disable_counters(req->ctx, req->set);
+}
+
+/**
+ * pfm_cell_disable_counters_smp
+ *
+ **/
+static void pfm_cell_disable_counters_smp(struct pfm_context *ctx,
+					  struct pfm_event_set *set)
+{
+	int target_cpu;
+	struct pfm_cell_remote_req req;
+
+	if (ctx->flags.not_dflt_ctxsw && !ctx->flags.system) {
+		target_cpu = get_target_cpu(ctx);
+		if (target_cpu == smp_processor_id()) {
+			pfm_cell_disable_counters(ctx, set);
+			return ;
+		}
+
+		if (irqs_disabled())
+			return ;
+
+		req.ctx = ctx;
+		req.set = set;
+		smp_call_function_single(target_cpu,
+					 pfm_cell_disable_counters_remote,
+					 &req, 0, 1);
+	} else {
+		pfm_cell_disable_counters(ctx, set);
+	}
+}
+
 /*
  * Return the thread id of the specified ppu signal.
  */
@@ -815,8 +1019,8 @@ static int get_ppu_signal_groups(struct 
  * The counter enable bit of the pmX_control PMC is enabled while the target
  * task runs on the target HW thread.
  **/
-void pfm_cell_restore_pmcs(struct pfm_context *ctx,
-			   struct pfm_event_set *set)
+static void pfm_cell_restore_pmcs(struct pfm_context *ctx,
+				  struct pfm_event_set *set)
 {
 	u64 ctr_ctrl;
 	u64 *used_pmcs = set->used_pmcs;
@@ -873,13 +1077,55 @@ void pfm_cell_restore_pmcs(struct pfm_co
 }
 
 /**
+ * pfm_cell_restore_pmcs_remote
+ *
+ * This function is the wrapper of pfm_cell_restore_pmcs
+ * to restore pmcs on the remote CPU(node).
+ **/
+static void pfm_cell_restore_pmcs_remote(void *arg)
+{
+	struct pfm_cell_remote_req *req = arg;
+	pfm_cell_restore_pmcs(req->ctx, req->set);
+}
+
+/**
+ * pfm_cell_restore_pmcs_smp
+ **/
+static void pfm_cell_restore_pmcs_smp(struct pfm_context *ctx,
+				      struct pfm_event_set *set)
+{
+	int target_cpu;
+	struct pfm_cell_remote_req req;
+
+	if (ctx->flags.not_dflt_ctxsw && !ctx->flags.system) {
+		target_cpu = get_target_cpu(ctx);
+		if (target_cpu == smp_processor_id()) {
+			pfm_cell_restore_pmcs(ctx, set);
+			return ;
+		}
+
+		if (irqs_disabled())
+			return ;
+
+		req.ctx = ctx;
+		req.set = set;
+		smp_call_function_single(target_cpu,
+					 pfm_cell_restore_pmcs_remote,
+					 &req, 0, 1);
+	} else {
+		pfm_cell_restore_pmcs(ctx, set);
+	}
+}
+
+
+/**
  * pfm_cell_restore_pmds
  *
  * Write to pm_control register before writing to counter registers
  * so that we can decide the counter width berfore writing to the couters.
  **/
-void pfm_cell_restore_pmds(struct pfm_context *ctx,
-			   struct pfm_event_set *set)
+static void pfm_cell_restore_pmds(struct pfm_context *ctx,
+				  struct pfm_event_set *set)
 {
 	u64 *used_pmds;
 	unsigned int i, max_pmd;
@@ -907,6 +1153,47 @@ void pfm_cell_restore_pmds(struct pfm_co
 }
 
 /**
+ * pfm_cell_restore_pmds_remote
+ *
+ * This function is the wrapper of pfm_cell_restore_pmds
+ * to restore pmds on the remote CPU(node).
+ **/
+static void pfm_cell_restore_pmds_remote(void *arg)
+{
+	struct pfm_cell_remote_req *req = arg;
+	pfm_cell_restore_pmds(req->ctx, req->set);
+}
+
+/**
+ * pfm_cell_restore_pmds_smp
+ *
+ **/
+static void pfm_cell_restore_pmds_smp(struct pfm_context *ctx,
+				      struct pfm_event_set *set)
+{
+	int target_cpu;
+	struct pfm_cell_remote_req req;
+
+	if (ctx->flags.not_dflt_ctxsw && !ctx->flags.system) {
+		target_cpu = get_target_cpu(ctx);
+		if (target_cpu == smp_processor_id()) {
+			pfm_cell_restore_pmds(ctx, set);
+			return ;
+		}
+		if (irqs_disabled())
+			return ;
+
+		req.ctx = ctx;
+		req.set = set;
+		smp_call_function_single(target_cpu,
+					 pfm_cell_restore_pmds_remote,
+					 &req, 0, 1);
+	} else {
+		pfm_cell_restore_pmds(ctx, set);
+	}
+}
+
+/**
  * pfm_cell_get_cntr_width
  *
  * This function check the 16bit counter field in pm_control pmc.
@@ -1180,20 +1467,98 @@ static void pfm_cell_unload_context(stru
 }
 
 /**
+ * pfm_cell_save_pmds
+ **/
+static void pfm_cell_save_pmds(struct pfm_context *ctx,
+			       struct pfm_event_set *set)
+{
+	u64 val, ovfl_mask;
+	u64 *used_pmds, *cnt_pmds;
+	u16 i, num;
+
+	ovfl_mask = pfm_pmu_conf->ovfl_mask;
+	num = set->nused_pmds;
+	cnt_pmds = pfm_pmu_conf->regs.cnt_pmds;
+	used_pmds = set->used_pmds;
+
+	/*
+	 * save HW PMD, for counters, reconstruct 64-bit value
+	 */
+	for (i = 0; num; i++) {
+		if (test_bit(i, cast_ulp(used_pmds))) {
+			val = pfm_cell_read_pmd(ctx, i);
+			if (likely(test_bit(i, cast_ulp(cnt_pmds))))
+				val = (set->pmds[i].value & ~ovfl_mask) |
+					(val & ovfl_mask);
+			set->pmds[i].value = val;
+			num--;
+		}
+	}
+}
+
+
+/**
  * pfm_cell_ctxswout_thread
  *
  * When a monitored thread is switched out (self-monitored or externally
  * monitored) we need to reset the debug-bus signals so the next context that
  * gets switched in can start from a clean set of signals.
  **/
-int pfm_cell_ctxswout_thread(struct task_struct *task,
-			     struct pfm_context *ctx, struct pfm_event_set *set)
+static int pfm_cell_ctxswout_thread(struct task_struct *task,
+				    struct pfm_context *ctx,
+				    struct pfm_event_set *set)
 {
+	if (pfm_arch_is_active(ctx))
+		pfm_cell_save_pmds(ctx, set);
+
 	reset_signals(smp_processor_id());
+
 	return 0;
 }
 
 /**
+ * pfm_cell_ctxswout_thread_remote
+ *
+ * This function is the wrapper of pfm_cell_ctxswout_thread
+ **/
+static void pfm_cell_ctxswout_thread_remote(void *arg)
+{
+	struct pfm_cell_remote_req *req = arg;
+	req->result = pfm_cell_ctxswout_thread(req->task, req->ctx, req->set);
+}
+
+/**
+ * pfm_cell_ctxswout_thread_smp
+ *
+ **/
+static int pfm_cell_ctxswout_thread_smp(struct task_struct *task,
+					struct pfm_context *ctx,
+					struct pfm_event_set *set)
+{
+	int target_cpu;
+	struct pfm_cell_remote_req req;
+
+	if (ctx->flags.not_dflt_ctxsw && !ctx->flags.system) {
+		target_cpu = get_target_cpu(ctx);
+		if (target_cpu == smp_processor_id())
+			return pfm_cell_ctxswout_thread(task, ctx, set);
+
+		if (irqs_disabled())
+			return 1;
+
+		req.task = task;
+		req.ctx = ctx;
+		req.set = set;
+		smp_call_function_single(target_cpu,
+					 pfm_cell_ctxswout_thread_remote,
+					 &req, 0, 1);
+		return req.result;
+	} else {
+		return pfm_cell_ctxswout_thread(task, ctx, set);
+	}
+}
+
+/**
  * pfm_cell_get_ovfl_pmds
  *
  * Determine which counters in this set have overflowed and fill in the
@@ -1245,6 +1610,46 @@ static void pfm_cell_get_ovfl_pmds(struc
 }
 
 /**
+ * pfm_cell_get_ovfl_pmds_remote
+ *
+ * This function is the wrapper of pfm_cell_get_ovfl_pmds
+ **/
+static void pfm_cell_get_ovfl_pmds_remote(void *arg)
+{
+	struct pfm_cell_remote_req *req = arg;
+	pfm_cell_get_ovfl_pmds(req->ctx, req->set);
+}
+
+/**
+ * pfm_cell_get_ovfl_pmds_smp
+ *
+ **/
+static void pfm_cell_get_ovfl_pmds_smp(struct pfm_context *ctx,
+				       struct pfm_event_set *set)
+{
+	int target_cpu;
+	struct pfm_cell_remote_req req;
+
+	if (ctx->flags.not_dflt_ctxsw && !ctx->flags.system) {
+		target_cpu = get_target_cpu(ctx);
+		if (target_cpu == smp_processor_id()) {
+			pfm_cell_get_ovfl_pmds(ctx, set);
+			return ;
+		}
+		if (irqs_disabled())
+			return ;
+
+		req.ctx = ctx;
+		req.set = set;
+		smp_call_function_single(target_cpu,
+					 pfm_cell_get_ovfl_pmds_remote,
+					 &req, 0, 1);
+	} else {
+		pfm_cell_get_ovfl_pmds(ctx, set);
+	}
+}
+
+/**
  * pfm_prepare_ctxswin_thread
  *
  **/
@@ -1260,6 +1665,8 @@ static void pfm_prepare_ctxswin_thread(s
 	last_pmc = NR_CTRS + 8;
 	list_for_each_entry(set, &ctx->set_list, list) {
 
+		set->priv_flags |= PFM_SETFL_PRIV_MOD_PMCS;
+		set->priv_flags |= PFM_SETFL_PRIV_MOD_PMDS;
 		used_pmcs = set->used_pmcs;
 		for (i = NR_CTRS; i < last_pmc; i++) {
 			if (!test_bit(i, used_pmcs))
@@ -1276,7 +1683,6 @@ static void pfm_prepare_ctxswin_thread(s
 			    signal_group < SIG_GROUP_EIB_BASE) {
 				set->pmcs[i] = update_sub_unit_field(
 					set->pmcs[i], spe_id);
-				set->priv_flags |= PFM_SETFL_PRIV_MOD_PMCS;
 				set->priv_flags &=
 					~PFM_SETFL_PRIV_WAIT_SUB_UNIT_UPDATE;
 				PFM_DBG("pmcs[%d] : 0x%lx", i, set->pmcs[i]);
@@ -1296,6 +1702,7 @@ static int pfm_spe_ctxsw_thread(struct n
 {
 	struct task_struct *p;
 	struct pfm_arch_context *ctx_arch;
+	struct spu *spu;
 	u64 spe_id;
 
 	p = pfm_get_task_by_pid(((struct spu *)arg)->ctx->tid);
@@ -1305,16 +1712,19 @@ static int pfm_spe_ctxsw_thread(struct n
 			return 0;
 		}
 
+		spu = arg;
 		ctx_arch = pfm_ctx_arch(p->pfm_context);
 		spe_id = pfm_get_spe_id(arg);
 		if (object_id) {
-			PFM_DBG("=== PFM SPE CTXSWIN === 0x%lx", spe_id);
+			PFM_DBG("=== PFM SPE CTXSWIN === 0x%lx node:%d",
+				spe_id, spu->node);
 			pfm_prepare_ctxswin_thread(p->pfm_context, spe_id);
 			ctx_arch->arg = arg;
 			pfm_ctxsw_in(NULL, p);
 
 		} else {
-			PFM_DBG("=== PFM SPE CTXSWOUT === 0x%lx", spe_id);
+			PFM_DBG("=== PFM SPE CTXSWOUT === 0x%lx node:%d",
+				spe_id, spu->node);
 			pfm_ctxsw_out(p, NULL);
 			ctx_arch->arg = NULL;
 		}
@@ -1577,15 +1987,15 @@ static struct pfm_arch_pmu_info pfm_cell
 	.create_context   = pfm_cell_create_context,
 	.free_context     = pfm_cell_free_context,
 	.write_pmc        = pfm_cell_write_pmc,
-	.write_pmd        = pfm_cell_write_pmd,
-	.read_pmd         = pfm_cell_read_pmd,
-	.enable_counters  = pfm_cell_enable_counters,
-	.disable_counters = pfm_cell_disable_counters,
+	.write_pmd        = pfm_cell_write_pmd_smp,
+	.read_pmd         = pfm_cell_read_pmd_smp,
+	.enable_counters  = pfm_cell_enable_counters_smp,
+	.disable_counters = pfm_cell_disable_counters_smp,
 	.irq_handler      = pfm_cell_irq_handler,
-	.get_ovfl_pmds    = pfm_cell_get_ovfl_pmds,
-	.restore_pmcs     = pfm_cell_restore_pmcs,
-	.restore_pmds     = pfm_cell_restore_pmds,
-	.ctxswout_thread  = pfm_cell_ctxswout_thread,
+	.get_ovfl_pmds    = pfm_cell_get_ovfl_pmds_smp,
+	.restore_pmcs     = pfm_cell_restore_pmcs_smp,
+	.restore_pmds     = pfm_cell_restore_pmds_smp,
+	.ctxswout_thread  = pfm_cell_ctxswout_thread_smp,
 	.load_context     = pfm_cell_load_context,
 	.unload_context   = pfm_cell_unload_context,
 };
