Subject: [patch] nmi-driven profiling for /proc/profile
From: Ingo Molnar <mingo@elte.hu>

nmi-driven profiling for /proc/profile

Signed-off-by: Ingo Molnar <mingo@elte.hu>
---
 arch/i386/kernel/crash.c   |    8 ---
 arch/i386/kernel/nmi.c     |   91 +++++++++++++++++++++++++++++++++++++++++----
 arch/x86_64/kernel/crash.c |    5 --
 arch/x86_64/kernel/irq.c   |    2 
 arch/x86_64/kernel/nmi.c   |   67 +++++++++++++++++++++++++++++++--
 include/asm-i386/apic.h    |    2 
 include/asm-x86_64/apic.h  |    2 
 include/linux/profile.h    |    1 
 kernel/profile.c           |    9 ++--
 kernel/time/tick-common.c  |    1 
 kernel/time/tick-sched.c   |    2 
 11 files changed, 159 insertions(+), 31 deletions(-)

Index: linux-2.6.23.1-rt5/arch/i386/kernel/crash.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/i386/kernel/crash.c
+++ linux-2.6.23.1-rt5/arch/i386/kernel/crash.c
@@ -70,14 +70,6 @@ static int crash_nmi_callback(struct not
 	return 1;
 }
 
-static void smp_send_nmi_allbutself(void)
-{
-	cpumask_t mask = cpu_online_map;
-	cpu_clear(safe_smp_processor_id(), mask);
-	if (!cpus_empty(mask))
-		send_IPI_mask(mask, NMI_VECTOR);
-}
-
 static struct notifier_block crash_nmi_nb = {
 	.notifier_call = crash_nmi_callback,
 };
Index: linux-2.6.23.1-rt5/arch/i386/kernel/nmi.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/i386/kernel/nmi.c
+++ linux-2.6.23.1-rt5/arch/i386/kernel/nmi.c
@@ -28,6 +28,8 @@
 #include <asm/smp.h>
 #include <asm/nmi.h>
 
+#include <asm/mach-default/mach_ipi.h>
+
 #include "mach_traps.h"
 
 int unknown_nmi_panic;
@@ -44,7 +46,7 @@ static cpumask_t backtrace_mask = CPU_MA
 atomic_t nmi_active = ATOMIC_INIT(0);		/* oprofile uses this */
 
 unsigned int nmi_watchdog = NMI_DEFAULT;
-static unsigned int nmi_hz = HZ;
+static unsigned int nmi_hz = 1000;
 
 static DEFINE_PER_CPU(short, wd_enabled);
 
@@ -95,7 +97,7 @@ static int __init check_nmi_watchdog(voi
 	for_each_possible_cpu(cpu)
 		prev_nmi_count[cpu] = per_cpu(irq_stat, cpu).__nmi_count;
 	local_irq_enable();
-	mdelay((20*1000)/nmi_hz); // wait 20 ticks
+	mdelay((100*1000)/nmi_hz); /* wait 100 ticks */
 
 	for_each_possible_cpu(cpu) {
 #ifdef CONFIG_SMP
@@ -319,9 +321,48 @@ EXPORT_SYMBOL(touch_nmi_watchdog);
 
 extern void die_nmi(struct pt_regs *, const char *msg);
 
-__kprobes int nmi_watchdog_tick(struct pt_regs * regs, unsigned reason)
+int nmi_show_regs[NR_CPUS];
+
+void nmi_show_all_regs(void)
 {
+	int i;
+
+	if (system_state == SYSTEM_BOOTING)
+		return;
+
+	printk(KERN_WARNING "nmi_show_all_regs(): start on CPU#%d.\n",
+		raw_smp_processor_id());
+	dump_stack();
+
+	for_each_online_cpu(i)
+		nmi_show_regs[i] = 1;
+
+	smp_send_nmi_allbutself();
+
+	for_each_online_cpu(i) {
+		while (nmi_show_regs[i] == 1)
+			barrier();
+	}
+}
+
+static DEFINE_SPINLOCK(nmi_print_lock);
+
+void irq_show_regs_callback(int cpu, struct pt_regs *regs)
+{
+	if (!nmi_show_regs[cpu])
+		return;
 
+	nmi_show_regs[cpu] = 0;
+	spin_lock(&nmi_print_lock);
+	printk(KERN_WARNING "NMI show regs on CPU#%d:\n", cpu);
+	printk(KERN_WARNING "apic_timer_irqs: %d\n",
+		per_cpu(irq_stat, cpu).apic_timer_irqs);
+	show_regs(regs);
+	spin_unlock(&nmi_print_lock);
+}
+
+__kprobes int nmi_watchdog_tick(struct pt_regs *regs, unsigned reason)
+{
 	/*
 	 * Since current_thread_info()-> is always on the stack, and we
 	 * always switch the stack NMI-atomically, it's safe to use
@@ -332,6 +373,8 @@ __kprobes int nmi_watchdog_tick(struct p
 	int cpu = smp_processor_id();
 	int rc=0;
 
+	__profile_tick(CPU_PROFILING, regs);
+
 	/* check for other users first */
 	if (notify_die(DIE_NMI, "nmi", regs, reason, 2, SIGINT)
 			== NOTIFY_STOP) {
@@ -355,6 +398,9 @@ __kprobes int nmi_watchdog_tick(struct p
 	 */
 	sum = per_cpu(irq_stat, cpu).apic_timer_irqs + kstat_cpu(cpu).irqs[0];
 
+	irq_show_regs_callback(cpu, regs);
+
+	/* if the apic timer isn't firing, this cpu isn't doing much */
 	/* if the none of the timers isn't firing, this cpu isn't doing much */
 	if (!touched && last_irq_sums[cpu] == sum) {
 		/*
@@ -362,11 +408,30 @@ __kprobes int nmi_watchdog_tick(struct p
 		 * wait a few IRQs (5 seconds) before doing the oops ...
 		 */
 		alert_counter[cpu]++;
-		if (alert_counter[cpu] == 5*nmi_hz)
-			/*
-			 * die_nmi will return ONLY if NOTIFY_STOP happens..
-			 */
-			die_nmi(regs, "BUG: NMI Watchdog detected LOCKUP");
+		if (alert_counter[cpu] && !(alert_counter[cpu] % (5*nmi_hz))) {
+			int i;
+
+			spin_lock(&nmi_print_lock);
+			printk(KERN_WARNING "NMI watchdog detected lockup on "
+				"CPU#%d (%d/%d)\n", cpu, alert_counter[cpu],
+						    5*nmi_hz);
+			show_regs(regs);
+			spin_unlock(&nmi_print_lock);
+
+			for_each_online_cpu(i) {
+				if (i == cpu)
+					continue;
+				nmi_show_regs[i] = 1;
+				while (nmi_show_regs[i] == 1)
+					cpu_relax();
+			}
+			printk(KERN_WARNING "NMI watchdog running again ...\n");
+			for_each_online_cpu(i)
+				alert_counter[i] = 0;
+
+
+		}
+
 	} else {
 		last_irq_sums[cpu] = sum;
 		alert_counter[cpu] = 0;
@@ -464,5 +529,15 @@ void __trigger_all_cpu_backtrace(void)
 	}
 }
 
+void smp_send_nmi_allbutself(void)
+{
+#ifdef CONFIG_SMP
+	cpumask_t mask = cpu_online_map;
+	cpu_clear(safe_smp_processor_id(), mask);
+	if (!cpus_empty(mask))
+		send_IPI_mask(mask, NMI_VECTOR);
+#endif
+}
+
 EXPORT_SYMBOL(nmi_active);
 EXPORT_SYMBOL(nmi_watchdog);
Index: linux-2.6.23.1-rt5/arch/x86_64/kernel/crash.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/x86_64/kernel/crash.c
+++ linux-2.6.23.1-rt5/arch/x86_64/kernel/crash.c
@@ -62,11 +62,6 @@ static int crash_nmi_callback(struct not
 	return 1;
 }
 
-static void smp_send_nmi_allbutself(void)
-{
-	send_IPI_allbutself(NMI_VECTOR);
-}
-
 /*
  * This code is a best effort heuristic to get the
  * other cpus to stop executing. So races with
Index: linux-2.6.23.1-rt5/arch/x86_64/kernel/irq.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/x86_64/kernel/irq.c
+++ linux-2.6.23.1-rt5/arch/x86_64/kernel/irq.c
@@ -111,6 +111,8 @@ asmlinkage unsigned int do_IRQ(struct pt
 	unsigned vector = ~regs->orig_rax;
 	unsigned irq;
 
+	irq_show_regs_callback(smp_processor_id(), regs);
+
 	exit_idle();
 	irq_enter();
 	irq = __get_cpu_var(vector_irq)[vector];
Index: linux-2.6.23.1-rt5/arch/x86_64/kernel/nmi.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/x86_64/kernel/nmi.c
+++ linux-2.6.23.1-rt5/arch/x86_64/kernel/nmi.c
@@ -22,11 +22,13 @@
 #include <linux/kprobes.h>
 #include <linux/cpumask.h>
 #include <linux/kdebug.h>
+#include <linux/kernel_stat.h>
 
 #include <asm/smp.h>
 #include <asm/nmi.h>
 #include <asm/proto.h>
 #include <asm/mce.h>
+#include <asm/mach_apic.h>
 
 int unknown_nmi_panic;
 int nmi_watchdog_enabled;
@@ -44,7 +46,7 @@ atomic_t nmi_active = ATOMIC_INIT(0);		/
 int panic_on_timeout;
 
 unsigned int nmi_watchdog = NMI_DEFAULT;
-static unsigned int nmi_hz = HZ;
+static unsigned int nmi_hz = 1000;
 
 static DEFINE_PER_CPU(short, wd_enabled);
 
@@ -302,7 +304,7 @@ void touch_nmi_watchdog(void)
 		unsigned cpu;
 
 		/*
- 		 * Tell other CPUs to reset their alert counters. We cannot
+		 * Tell other CPUs to reset their alert counters. We cannot
 		 * do it ourselves because the alert count increase is not
 		 * atomic.
 		 */
@@ -312,7 +314,42 @@ void touch_nmi_watchdog(void)
 		}
 	}
 
- 	touch_softlockup_watchdog();
+	touch_softlockup_watchdog();
+}
+
+int nmi_show_regs[NR_CPUS];
+
+void nmi_show_all_regs(void)
+{
+	int i;
+
+	if (system_state == SYSTEM_BOOTING)
+		return;
+
+	smp_send_nmi_allbutself();
+
+	for_each_online_cpu(i)
+		nmi_show_regs[i] = 1;
+
+	for_each_online_cpu(i) {
+		while (nmi_show_regs[i] == 1)
+			barrier();
+	}
+}
+
+static DEFINE_SPINLOCK(nmi_print_lock);
+
+void irq_show_regs_callback(int cpu, struct pt_regs *regs)
+{
+	if (!nmi_show_regs[cpu])
+		return;
+
+	nmi_show_regs[cpu] = 0;
+	spin_lock(&nmi_print_lock);
+	printk(KERN_WARNING "NMI show regs on CPU#%d:\n", cpu);
+	printk(KERN_WARNING "apic_timer_irqs: %d\n", read_pda(apic_timer_irqs));
+	show_regs(regs);
+	spin_unlock(&nmi_print_lock);
 }
 
 int __kprobes nmi_watchdog_tick(struct pt_regs * regs, unsigned reason)
@@ -322,6 +359,9 @@ int __kprobes nmi_watchdog_tick(struct p
 	int cpu = smp_processor_id();
 	int rc = 0;
 
+	irq_show_regs_callback(cpu, regs);
+	__profile_tick(CPU_PROFILING, regs);
+
 	/* check for other users first */
 	if (notify_die(DIE_NMI, "nmi", regs, reason, 2, SIGINT)
 			== NOTIFY_STOP) {
@@ -330,6 +370,7 @@ int __kprobes nmi_watchdog_tick(struct p
 	}
 
 	sum = read_pda(apic_timer_irqs);
+
 	if (__get_cpu_var(nmi_touch)) {
 		__get_cpu_var(nmi_touch) = 0;
 		touched = 1;
@@ -358,9 +399,20 @@ int __kprobes nmi_watchdog_tick(struct p
 		 * wait a few IRQs (5 seconds) before doing the oops ...
 		 */
 		local_inc(&__get_cpu_var(alert_counter));
-		if (local_read(&__get_cpu_var(alert_counter)) == 5*nmi_hz)
+		if (local_read(&__get_cpu_var(alert_counter)) == 5*nmi_hz) {
+			int i;
+
+			for_each_online_cpu(i) {
+				if (i == cpu)
+					continue;
+				nmi_show_regs[i] = 1;
+				while (nmi_show_regs[i] == 1)
+					cpu_relax();
+			}
+
 			die_nmi("NMI Watchdog detected LOCKUP on CPU %d\n", regs,
 				panic_on_timeout);
+		}
 	} else {
 		__get_cpu_var(last_irq_sum) = sum;
 		local_set(&__get_cpu_var(alert_counter), 0);
@@ -478,6 +530,13 @@ void __trigger_all_cpu_backtrace(void)
 	}
 }
 
+void smp_send_nmi_allbutself(void)
+{
+#ifdef CONFIG_SMP
+	send_IPI_allbutself(NMI_VECTOR);
+#endif
+}
+
 EXPORT_SYMBOL(nmi_active);
 EXPORT_SYMBOL(nmi_watchdog);
 EXPORT_SYMBOL(touch_nmi_watchdog);
Index: linux-2.6.23.1-rt5/include/asm-i386/apic.h
===================================================================
--- linux-2.6.23.1-rt5.orig/include/asm-i386/apic.h
+++ linux-2.6.23.1-rt5/include/asm-i386/apic.h
@@ -118,6 +118,8 @@ extern int local_apic_timer_c2_ok;
 
 extern int local_apic_timer_disabled;
 
+extern void smp_send_nmi_allbutself(void);
+
 #else /* !CONFIG_X86_LOCAL_APIC */
 static inline void lapic_shutdown(void) { }
 
Index: linux-2.6.23.1-rt5/include/asm-x86_64/apic.h
===================================================================
--- linux-2.6.23.1-rt5.orig/include/asm-x86_64/apic.h
+++ linux-2.6.23.1-rt5/include/asm-x86_64/apic.h
@@ -86,6 +86,8 @@ extern void setup_APIC_extended_lvt(unsi
 
 extern int apic_is_clustered_box(void);
 
+extern void smp_send_nmi_allbutself(void);
+
 #define K8_APIC_EXT_LVT_BASE    0x500
 #define K8_APIC_EXT_INT_MSG_FIX 0x0
 #define K8_APIC_EXT_INT_MSG_SMI 0x2
Index: linux-2.6.23.1-rt5/include/linux/profile.h
===================================================================
--- linux-2.6.23.1-rt5.orig/include/linux/profile.h
+++ linux-2.6.23.1-rt5/include/linux/profile.h
@@ -23,6 +23,7 @@ struct notifier_block;
 
 /* init basic kernel profiler */
 void __init profile_init(void);
+void __profile_tick(int type, struct pt_regs *regs);
 void profile_tick(int);
 
 /*
Index: linux-2.6.23.1-rt5/kernel/profile.c
===================================================================
--- linux-2.6.23.1-rt5.orig/kernel/profile.c
+++ linux-2.6.23.1-rt5/kernel/profile.c
@@ -407,16 +407,19 @@ void profile_hits(int type, void *__pc, 
 
 EXPORT_SYMBOL_GPL(profile_hits);
 
-void profile_tick(int type)
+void __profile_tick(int type, struct pt_regs *regs)
 {
-	struct pt_regs *regs = get_irq_regs();
-
 	if (type == CPU_PROFILING && timer_hook)
 		timer_hook(regs);
 	if (!user_mode(regs) && cpu_isset(smp_processor_id(), prof_cpu_mask))
 		profile_hit(type, (void *)profile_pc(regs));
 }
 
+void profile_tick(int type)
+{
+	return __profile_tick(type, get_irq_regs());
+}
+
 #ifdef CONFIG_PROC_FS
 #include <linux/proc_fs.h>
 #include <asm/uaccess.h>
Index: linux-2.6.23.1-rt5/kernel/time/tick-common.c
===================================================================
--- linux-2.6.23.1-rt5.orig/kernel/time/tick-common.c
+++ linux-2.6.23.1-rt5/kernel/time/tick-common.c
@@ -68,7 +68,6 @@ static void tick_periodic(int cpu)
 	}
 
 	update_process_times(user_mode(get_irq_regs()));
-	profile_tick(CPU_PROFILING);
 }
 
 /*
Index: linux-2.6.23.1-rt5/kernel/time/tick-sched.c
===================================================================
--- linux-2.6.23.1-rt5.orig/kernel/time/tick-sched.c
+++ linux-2.6.23.1-rt5/kernel/time/tick-sched.c
@@ -440,7 +440,6 @@ static void tick_nohz_handler(struct clo
 	}
 
 	update_process_times(user_mode(regs));
-	profile_tick(CPU_PROFILING);
 
 	/* Do not restart, when we are in the idle loop */
 	if (ts->tick_stopped)
@@ -554,7 +553,6 @@ static enum hrtimer_restart tick_sched_t
 		 */
 		spin_unlock(&base->lock);
 		update_process_times(user_mode(regs));
-		profile_tick(CPU_PROFILING);
 		spin_lock(&base->lock);
 	}
 
