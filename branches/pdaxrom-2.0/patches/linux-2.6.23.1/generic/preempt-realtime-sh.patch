From lethal@linux-sh.org Fri Apr 27 10:21:47 2007
Date: Fri, 27 Apr 2007 10:21:47 +0900
From: Paul Mundt <lethal@linux-sh.org>
To: Thomas Gleixner <tglx@linutronix.de>, Ingo Molnar <mingo@elte.hu>
Subject: [PATCH] preempt-rt: Preliminary SH support

Hi Thomas, Ingo,

Here's preliminary preempt-rt support for SH. It was written against
2.6.21-rc5, but still applies cleanly. I've kept the clock events stuff
out of this patch, since I'm planning on overhauling the timer stuff on
SH first, but this should trickle in through 2.6.22-rc.

Feel free to either merge this in to preempt-rt or hold off until the
timer stuff gets done.

Patch from Matsubara-san.

Signed-off-by: Katsuya MATSUBARA <matsu@igel.co.jp>
Signed-off-by: Paul Mundt <lethal@linux-sh.org>

--

 arch/sh/kernel/cpu/clock.c        |    2 -
 arch/sh/kernel/cpu/sh4/sq.c       |    2 -
 arch/sh/kernel/entry-common.S     |    8 ++--
 arch/sh/kernel/irq.c              |    2 -
 arch/sh/kernel/process.c          |   10 +++---
 arch/sh/kernel/semaphore.c        |   14 ++++++--
 arch/sh/kernel/sh_ksyms.c         |    9 ++---
 arch/sh/kernel/signal.c           |    7 ++++
 arch/sh/kernel/time.c             |    2 -
 arch/sh/kernel/traps.c            |    2 -
 arch/sh/mm/cache-sh4.c            |   12 +++----
 arch/sh/mm/init.c                 |    2 -
 arch/sh/mm/pg-sh4.c               |    4 +-
 arch/sh/mm/tlb-flush.c            |   20 ++++++------
 arch/sh/mm/tlb-sh4.c              |    4 +-
 include/asm-sh/atomic-irq.h       |   24 +++++++-------
 include/asm-sh/atomic.h           |    8 ++--
 include/asm-sh/bitops.h           |   24 +++++++-------
 include/asm-sh/pgalloc.h          |    2 -
 include/asm-sh/rwsem.h            |   46 ++++++++++++++--------------
 include/asm-sh/semaphore-helper.h |    8 ++--
 include/asm-sh/semaphore.h        |   61 +++++++++++++++++++++++---------------
 include/asm-sh/system.h           |   12 +++----
 include/asm-sh/thread_info.h      |    2 +
 24 files changed, 158 insertions(+), 129 deletions(-)

Index: linux-2.6.23.1-rt5/arch/sh/kernel/cpu/clock.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/sh/kernel/cpu/clock.c
+++ linux-2.6.23.1-rt5/arch/sh/kernel/cpu/clock.c
@@ -28,7 +28,7 @@
 #include <asm/timer.h>
 
 static LIST_HEAD(clock_list);
-static DEFINE_SPINLOCK(clock_lock);
+static DEFINE_RAW_SPINLOCK(clock_lock);
 static DEFINE_MUTEX(clock_list_sem);
 
 /*
Index: linux-2.6.23.1-rt5/arch/sh/kernel/cpu/sh4/sq.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/sh/kernel/cpu/sh4/sq.c
+++ linux-2.6.23.1-rt5/arch/sh/kernel/cpu/sh4/sq.c
@@ -37,7 +37,7 @@ struct sq_mapping {
 };
 
 static struct sq_mapping *sq_mapping_list;
-static DEFINE_SPINLOCK(sq_mapping_lock);
+static DEFINE_RAW_SPINLOCK(sq_mapping_lock);
 static struct kmem_cache *sq_cache;
 static unsigned long *sq_bitmap;
 
Index: linux-2.6.23.1-rt5/arch/sh/kernel/entry-common.S
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/sh/kernel/entry-common.S
+++ linux-2.6.23.1-rt5/arch/sh/kernel/entry-common.S
@@ -157,7 +157,7 @@ ENTRY(resume_userspace)
 	mov.l	@(TI_FLAGS,r8), r0		! current_thread_info->flags
 	tst	#_TIF_WORK_MASK, r0
 	bt/s	__restore_all
-	 tst	#_TIF_NEED_RESCHED, r0
+	 tst	#_TIF_NEED_RESCHED|_TIF_NEED_RESCHED_DELAYED, r0
 
 	.align	2
 work_pending:
@@ -209,10 +209,10 @@ work_resched:
 	tst	#_TIF_WORK_MASK, r0
 	bt	__restore_all
 	bra	work_pending
-	 tst	#_TIF_NEED_RESCHED, r0
+	 tst	#_TIF_NEED_RESCHED | _TIF_NEED_RESCHED_DELAYED, r0
 
 	.align	2
-1:	.long	schedule
+1:	.long	__schedule
 2:	.long	do_notify_resume
 3:	.long	restore_all
 #ifdef CONFIG_TRACE_IRQFLAGS
@@ -226,7 +226,7 @@ syscall_exit_work:
 	! r8: current_thread_info
 	tst	#_TIF_SYSCALL_TRACE | _TIF_SINGLESTEP, r0
 	bt/s	work_pending
-	 tst	#_TIF_NEED_RESCHED, r0
+	 tst	#_TIF_NEED_RESCHED| _TIF_NEED_RESCHED_DELAYED, r0
 #ifdef CONFIG_TRACE_IRQFLAGS
 	mov.l	5f, r0
 	jsr	@r0
Index: linux-2.6.23.1-rt5/arch/sh/kernel/irq.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/sh/kernel/irq.c
+++ linux-2.6.23.1-rt5/arch/sh/kernel/irq.c
@@ -82,7 +82,7 @@ static union irq_ctx *hardirq_ctx[NR_CPU
 static union irq_ctx *softirq_ctx[NR_CPUS] __read_mostly;
 #endif
 
-asmlinkage int do_IRQ(unsigned int irq, struct pt_regs *regs)
+asmlinkage notrace int do_IRQ(unsigned int irq, struct pt_regs *regs)
 {
 	struct pt_regs *old_regs = set_irq_regs(regs);
 #ifdef CONFIG_4KSTACKS
Index: linux-2.6.23.1-rt5/arch/sh/kernel/process.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/sh/kernel/process.c
+++ linux-2.6.23.1-rt5/arch/sh/kernel/process.c
@@ -64,7 +64,7 @@ void default_idle(void)
 		clear_thread_flag(TIF_POLLING_NRFLAG);
 		smp_mb__after_clear_bit();
 		set_bl_bit();
-		while (!need_resched())
+		while (!need_resched() && !need_resched_delayed())
 			cpu_sleep();
 		clear_bl_bit();
 		set_thread_flag(TIF_POLLING_NRFLAG);
@@ -85,13 +85,15 @@ void cpu_idle(void)
 			idle = default_idle;
 
 		tick_nohz_stop_sched_tick();
-		while (!need_resched())
+		while (!need_resched() && !need_resched_delayed())
 			idle();
 		tick_nohz_restart_sched_tick();
 
-		preempt_enable_no_resched();
-		schedule();
+		local_irq_disable();
+		__preempt_enable_no_resched();
+		__schedule();
 		preempt_disable();
+		local_irq_enable();
 		check_pgt_cache();
 	}
 }
Index: linux-2.6.23.1-rt5/arch/sh/kernel/semaphore.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/sh/kernel/semaphore.c
+++ linux-2.6.23.1-rt5/arch/sh/kernel/semaphore.c
@@ -46,7 +46,7 @@ DEFINE_SPINLOCK(semaphore_wake_lock);
  * critical part is the inline stuff in <asm/semaphore.h>
  * where we want to avoid any extra jumps and calls.
  */
-void __up(struct semaphore *sem)
+void __attribute_used__ __compat_up(struct compat_semaphore *sem)
 {
 	wake_one_more(sem);
 	wake_up(&sem->wait);
@@ -104,7 +104,7 @@ void __up(struct semaphore *sem)
 	tsk->state = TASK_RUNNING;		\
 	remove_wait_queue(&sem->wait, &wait);
 
-void __sched __down(struct semaphore * sem)
+void __attribute_used__ __sched __compat_down(struct compat_semaphore * sem)
 {
 	DOWN_VAR
 	DOWN_HEAD(TASK_UNINTERRUPTIBLE)
@@ -114,7 +114,7 @@ void __sched __down(struct semaphore * s
 	DOWN_TAIL(TASK_UNINTERRUPTIBLE)
 }
 
-int __sched __down_interruptible(struct semaphore * sem)
+int __attribute_used__ __sched __compat_down_interruptible(struct compat_semaphore * sem)
 {
 	int ret = 0;
 	DOWN_VAR
@@ -133,7 +133,13 @@ int __sched __down_interruptible(struct 
 	return ret;
 }
 
-int __down_trylock(struct semaphore * sem)
+int __attribute_used__ __compat_down_trylock(struct compat_semaphore * sem)
 {
 	return waking_non_zero_trylock(sem);
 }
+
+fastcall int __sched compat_sem_is_locked(struct compat_semaphore *sem)
+{
+  return (int) atomic_read(&sem->count) < 0;
+}
+
Index: linux-2.6.23.1-rt5/arch/sh/kernel/sh_ksyms.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/sh/kernel/sh_ksyms.c
+++ linux-2.6.23.1-rt5/arch/sh/kernel/sh_ksyms.c
@@ -26,7 +26,6 @@ EXPORT_SYMBOL(sh_mv);
 /* platform dependent support */
 EXPORT_SYMBOL(dump_fpu);
 EXPORT_SYMBOL(kernel_thread);
-EXPORT_SYMBOL(irq_desc);
 EXPORT_SYMBOL(no_irq_type);
 
 EXPORT_SYMBOL(strlen);
@@ -50,9 +49,9 @@ EXPORT_SYMBOL(get_vm_area);
 #endif
 
 /* semaphore exports */
-EXPORT_SYMBOL(__up);
-EXPORT_SYMBOL(__down);
-EXPORT_SYMBOL(__down_interruptible);
+EXPORT_SYMBOL(__compat_up);
+EXPORT_SYMBOL(__compat_down);
+EXPORT_SYMBOL(__compat_down_interruptible);
 
 EXPORT_SYMBOL(__udelay);
 EXPORT_SYMBOL(__ndelay);
@@ -141,7 +140,7 @@ EXPORT_SYMBOL(__flush_purge_region);
 EXPORT_SYMBOL(clear_user_page);
 #endif
 
-EXPORT_SYMBOL(__down_trylock);
+EXPORT_SYMBOL(__compat_down_trylock);
 
 #ifdef CONFIG_SMP
 EXPORT_SYMBOL(synchronize_irq);
Index: linux-2.6.23.1-rt5/arch/sh/kernel/signal.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/sh/kernel/signal.c
+++ linux-2.6.23.1-rt5/arch/sh/kernel/signal.c
@@ -566,6 +566,13 @@ static void do_signal(struct pt_regs *re
 	struct k_sigaction ka;
 	sigset_t *oldset;
 
+#ifdef CONFIG_PREEMPT_RT
+        /*
+         * Fully-preemptible kernel does not need interrupts disabled:
+         */
+        raw_local_irq_enable();
+        preempt_check_resched();
+#endif
 	/*
 	 * We want the common case to go fast, which
 	 * is why we may in certain cases get here from
Index: linux-2.6.23.1-rt5/arch/sh/kernel/time.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/sh/kernel/time.c
+++ linux-2.6.23.1-rt5/arch/sh/kernel/time.c
@@ -24,7 +24,7 @@
 struct sys_timer *sys_timer;
 
 /* Move this somewhere more sensible.. */
-DEFINE_SPINLOCK(rtc_lock);
+DEFINE_RAW_SPINLOCK(rtc_lock);
 EXPORT_SYMBOL(rtc_lock);
 
 /* Dummy RTC ops */
Index: linux-2.6.23.1-rt5/arch/sh/kernel/traps.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/sh/kernel/traps.c
+++ linux-2.6.23.1-rt5/arch/sh/kernel/traps.c
@@ -77,7 +77,7 @@ static void dump_mem(const char *str, un
 	}
 }
 
-static DEFINE_SPINLOCK(die_lock);
+static DEFINE_RAW_SPINLOCK(die_lock);
 
 void die(const char * str, struct pt_regs * regs, long err)
 {
Index: linux-2.6.23.1-rt5/arch/sh/mm/cache-sh4.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/sh/mm/cache-sh4.c
+++ linux-2.6.23.1-rt5/arch/sh/mm/cache-sh4.c
@@ -189,7 +189,7 @@ void flush_cache_sigtramp(unsigned long 
 	index = CACHE_IC_ADDRESS_ARRAY |
 			(v & current_cpu_data.icache.entry_mask);
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	jump_to_P2();
 
 	for (i = 0; i < current_cpu_data.icache.ways;
@@ -198,7 +198,7 @@ void flush_cache_sigtramp(unsigned long 
 
 	back_to_P1();
 	wmb();
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 static inline void flush_cache_4096(unsigned long start,
@@ -214,10 +214,10 @@ static inline void flush_cache_4096(unsi
 	    (start < CACHE_OC_ADDRESS_ARRAY))
 		exec_offset = 0x20000000;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	__flush_cache_4096(start | SH_CACHE_ASSOC,
 			   P1SEGADDR(phys), exec_offset);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 /*
@@ -245,7 +245,7 @@ static inline void flush_icache_all(void
 {
 	unsigned long flags, ccr;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	jump_to_P2();
 
 	/* Flush I-cache */
@@ -259,7 +259,7 @@ static inline void flush_icache_all(void
 	 */
 
 	back_to_P1();
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 void flush_dcache_all(void)
Index: linux-2.6.23.1-rt5/arch/sh/mm/init.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/sh/mm/init.c
+++ linux-2.6.23.1-rt5/arch/sh/mm/init.c
@@ -21,7 +21,7 @@
 #include <asm/sections.h>
 #include <asm/cache.h>
 
-DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
+DEFINE_PER_CPU_LOCKED(struct mmu_gather, mmu_gathers);
 pgd_t swapper_pg_dir[PTRS_PER_PGD];
 
 void (*copy_page)(void *from, void *to);
Index: linux-2.6.23.1-rt5/arch/sh/mm/pg-sh4.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/sh/mm/pg-sh4.c
+++ linux-2.6.23.1-rt5/arch/sh/mm/pg-sh4.c
@@ -26,9 +26,9 @@ static inline void *kmap_coherent(struct
 	vaddr = __fix_to_virt(FIX_CMAP_END - idx);
 	pte = mk_pte(page, PAGE_KERNEL);
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	flush_tlb_one(get_asid(), vaddr);
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	update_mmu_cache(NULL, vaddr, pte);
 
Index: linux-2.6.23.1-rt5/arch/sh/mm/tlb-flush.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/sh/mm/tlb-flush.c
+++ linux-2.6.23.1-rt5/arch/sh/mm/tlb-flush.c
@@ -24,7 +24,7 @@ void local_flush_tlb_page(struct vm_area
 		asid = cpu_asid(cpu, vma->vm_mm);
 		page &= PAGE_MASK;
 
-		local_irq_save(flags);
+		raw_local_irq_save(flags);
 		if (vma->vm_mm != current->mm) {
 			saved_asid = get_asid();
 			set_asid(asid);
@@ -32,7 +32,7 @@ void local_flush_tlb_page(struct vm_area
 		local_flush_tlb_one(asid, page);
 		if (saved_asid != MMU_NO_ASID)
 			set_asid(saved_asid);
-		local_irq_restore(flags);
+		raw_local_irq_restore(flags);
 	}
 }
 
@@ -46,7 +46,7 @@ void local_flush_tlb_range(struct vm_are
 		unsigned long flags;
 		int size;
 
-		local_irq_save(flags);
+		raw_local_irq_save(flags);
 		size = (end - start + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
 		if (size > (MMU_NTLB_ENTRIES/4)) { /* Too many TLB to flush */
 			cpu_context(cpu, mm) = NO_CONTEXT;
@@ -71,7 +71,7 @@ void local_flush_tlb_range(struct vm_are
 			if (saved_asid != MMU_NO_ASID)
 				set_asid(saved_asid);
 		}
-		local_irq_restore(flags);
+		raw_local_irq_restore(flags);
 	}
 }
 
@@ -81,7 +81,7 @@ void local_flush_tlb_kernel_range(unsign
 	unsigned long flags;
 	int size;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	size = (end - start + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
 	if (size > (MMU_NTLB_ENTRIES/4)) { /* Too many TLB to flush */
 		local_flush_tlb_all();
@@ -100,7 +100,7 @@ void local_flush_tlb_kernel_range(unsign
 		}
 		set_asid(saved_asid);
 	}
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 void local_flush_tlb_mm(struct mm_struct *mm)
@@ -112,11 +112,11 @@ void local_flush_tlb_mm(struct mm_struct
 	if (cpu_context(cpu, mm) != NO_CONTEXT) {
 		unsigned long flags;
 
-		local_irq_save(flags);
+		raw_local_irq_save(flags);
 		cpu_context(cpu, mm) = NO_CONTEXT;
 		if (mm == current->mm)
 			activate_context(mm, cpu);
-		local_irq_restore(flags);
+		raw_local_irq_restore(flags);
 	}
 }
 
@@ -131,10 +131,10 @@ void local_flush_tlb_all(void)
 	 *	TF-bit for SH-3, TI-bit for SH-4.
 	 *      It's same position, bit #2.
 	 */
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	status = ctrl_inl(MMUCR);
 	status |= 0x04;
 	ctrl_outl(status, MMUCR);
 	ctrl_barrier();
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
Index: linux-2.6.23.1-rt5/arch/sh/mm/tlb-sh4.c
===================================================================
--- linux-2.6.23.1-rt5.orig/arch/sh/mm/tlb-sh4.c
+++ linux-2.6.23.1-rt5/arch/sh/mm/tlb-sh4.c
@@ -51,7 +51,7 @@ void update_mmu_cache(struct vm_area_str
 		}
 	}
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 
 	/* Set PTEH register */
 	vpn = (address & MMU_VPN_MASK) | get_asid();
@@ -74,7 +74,7 @@ void update_mmu_cache(struct vm_area_str
 
 	/* Load the TLB */
 	asm volatile("ldtlb": /* no output */ : /* no input */ : "memory");
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 void local_flush_tlb_one(unsigned long asid, unsigned long page)
Index: linux-2.6.23.1-rt5/include/asm-sh/atomic-irq.h
===================================================================
--- linux-2.6.23.1-rt5.orig/include/asm-sh/atomic-irq.h
+++ linux-2.6.23.1-rt5/include/asm-sh/atomic-irq.h
@@ -10,29 +10,29 @@ static inline void atomic_add(int i, ato
 {
 	unsigned long flags;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	*(long *)v += i;
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 static inline void atomic_sub(int i, atomic_t *v)
 {
 	unsigned long flags;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	*(long *)v -= i;
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 static inline int atomic_add_return(int i, atomic_t *v)
 {
 	unsigned long temp, flags;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	temp = *(long *)v;
 	temp += i;
 	*(long *)v = temp;
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	return temp;
 }
@@ -41,11 +41,11 @@ static inline int atomic_sub_return(int 
 {
 	unsigned long temp, flags;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	temp = *(long *)v;
 	temp -= i;
 	*(long *)v = temp;
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	return temp;
 }
@@ -54,18 +54,18 @@ static inline void atomic_clear_mask(uns
 {
 	unsigned long flags;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	*(long *)v &= ~mask;
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 static inline void atomic_set_mask(unsigned int mask, atomic_t *v)
 {
 	unsigned long flags;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	*(long *)v |= mask;
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 #endif /* __ASM_SH_ATOMIC_IRQ_H */
Index: linux-2.6.23.1-rt5/include/asm-sh/atomic.h
===================================================================
--- linux-2.6.23.1-rt5.orig/include/asm-sh/atomic.h
+++ linux-2.6.23.1-rt5/include/asm-sh/atomic.h
@@ -49,11 +49,11 @@ static inline int atomic_cmpxchg(atomic_
 	int ret;
 	unsigned long flags;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	ret = v->counter;
 	if (likely(ret == old))
 		v->counter = new;
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	return ret;
 }
@@ -65,11 +65,11 @@ static inline int atomic_add_unless(atom
 	int ret;
 	unsigned long flags;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	ret = v->counter;
 	if (ret != u)
 		v->counter += a;
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	return ret != u;
 }
Index: linux-2.6.23.1-rt5/include/asm-sh/bitops.h
===================================================================
--- linux-2.6.23.1-rt5.orig/include/asm-sh/bitops.h
+++ linux-2.6.23.1-rt5/include/asm-sh/bitops.h
@@ -14,9 +14,9 @@ static inline void set_bit(int nr, volat
 
 	a += nr >> 5;
 	mask = 1 << (nr & 0x1f);
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	*a |= mask;
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 /*
@@ -32,9 +32,9 @@ static inline void clear_bit(int nr, vol
 
 	a += nr >> 5;
 	mask = 1 << (nr & 0x1f);
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	*a &= ~mask;
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 static inline void change_bit(int nr, volatile void * addr)
@@ -45,9 +45,9 @@ static inline void change_bit(int nr, vo
 
 	a += nr >> 5;
 	mask = 1 << (nr & 0x1f);
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	*a ^= mask;
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 }
 
 static inline int test_and_set_bit(int nr, volatile void * addr)
@@ -58,10 +58,10 @@ static inline int test_and_set_bit(int n
 
 	a += nr >> 5;
 	mask = 1 << (nr & 0x1f);
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	retval = (mask & *a) != 0;
 	*a |= mask;
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	return retval;
 }
@@ -74,10 +74,10 @@ static inline int test_and_clear_bit(int
 
 	a += nr >> 5;
 	mask = 1 << (nr & 0x1f);
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	retval = (mask & *a) != 0;
 	*a &= ~mask;
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	return retval;
 }
@@ -90,10 +90,10 @@ static inline int test_and_change_bit(in
 
 	a += nr >> 5;
 	mask = 1 << (nr & 0x1f);
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	retval = (mask & *a) != 0;
 	*a ^= mask;
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	return retval;
 }
Index: linux-2.6.23.1-rt5/include/asm-sh/pgalloc.h
===================================================================
--- linux-2.6.23.1-rt5.orig/include/asm-sh/pgalloc.h
+++ linux-2.6.23.1-rt5/include/asm-sh/pgalloc.h
@@ -13,7 +13,7 @@ static inline void pmd_populate_kernel(s
 	set_pmd(pmd, __pmd((unsigned long)pte));
 }
 
-static inline void pmd_populate(struct mm_struct *mm, pmd_t *pmd,
+static inline void notrace pmd_populate(struct mm_struct *mm, pmd_t *pmd,
 				struct page *pte)
 {
 	set_pmd(pmd, __pmd((unsigned long)page_address(pte)));
Index: linux-2.6.23.1-rt5/include/asm-sh/rwsem.h
===================================================================
--- linux-2.6.23.1-rt5.orig/include/asm-sh/rwsem.h
+++ linux-2.6.23.1-rt5/include/asm-sh/rwsem.h
@@ -19,7 +19,7 @@
 /*
  * the semaphore definition
  */
-struct rw_semaphore {
+struct compat_rw_semaphore {
 	long		count;
 #define RWSEM_UNLOCKED_VALUE		0x00000000
 #define RWSEM_ACTIVE_BIAS		0x00000001
@@ -27,7 +27,7 @@ struct rw_semaphore {
 #define RWSEM_WAITING_BIAS		(-0x00010000)
 #define RWSEM_ACTIVE_READ_BIAS		RWSEM_ACTIVE_BIAS
 #define RWSEM_ACTIVE_WRITE_BIAS		(RWSEM_WAITING_BIAS + RWSEM_ACTIVE_BIAS)
-	spinlock_t		wait_lock;
+	raw_spinlock_t		wait_lock;
 	struct list_head	wait_list;
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	struct lockdep_map	dep_map;
@@ -45,25 +45,25 @@ struct rw_semaphore {
 	  LIST_HEAD_INIT((name).wait_list) \
 	  __RWSEM_DEP_MAP_INIT(name) }
 
-#define DECLARE_RWSEM(name)		\
-	struct rw_semaphore name = __RWSEM_INITIALIZER(name)
+#define COMPAT_DECLARE_RWSEM(name)		\
+	struct compat_rw_semaphore name = __RWSEM_INITIALIZER(name)
 
-extern struct rw_semaphore *rwsem_down_read_failed(struct rw_semaphore *sem);
-extern struct rw_semaphore *rwsem_down_write_failed(struct rw_semaphore *sem);
-extern struct rw_semaphore *rwsem_wake(struct rw_semaphore *sem);
-extern struct rw_semaphore *rwsem_downgrade_wake(struct rw_semaphore *sem);
+extern struct compat_rw_semaphore *rwsem_down_read_failed(struct compat_rw_semaphore *sem);
+extern struct compat_rw_semaphore *rwsem_down_write_failed(struct compat_rw_semaphore *sem);
+extern struct compat_rw_semaphore *rwsem_wake(struct compat_rw_semaphore *sem);
+extern struct compat_rw_semaphore *rwsem_downgrade_wake(struct compat_rw_semaphore *sem);
 
-extern void __init_rwsem(struct rw_semaphore *sem, const char *name,
+extern void __compat_init_rwsem(struct rw_semaphore *sem, const char *name,
 			 struct lock_class_key *key);
 
-#define init_rwsem(sem)				\
+#define compat_init_rwsem(sem)				\
 do {						\
 	static struct lock_class_key __key;	\
 						\
-	__init_rwsem((sem), #sem, &__key);	\
+	__compat_init_rwsem((sem), #sem, &__key);	\
 } while (0)
 
-static inline void init_rwsem(struct rw_semaphore *sem)
+static inline void compat_init_rwsem(struct rw_semaphore *sem)
 {
 	sem->count = RWSEM_UNLOCKED_VALUE;
 	spin_lock_init(&sem->wait_lock);
@@ -73,7 +73,7 @@ static inline void init_rwsem(struct rw_
 /*
  * lock for reading
  */
-static inline void __down_read(struct rw_semaphore *sem)
+static inline void __down_read(struct compat_rw_semaphore *sem)
 {
 	if (atomic_inc_return((atomic_t *)(&sem->count)) > 0)
 		smp_wmb();
@@ -81,7 +81,7 @@ static inline void __down_read(struct rw
 		rwsem_down_read_failed(sem);
 }
 
-static inline int __down_read_trylock(struct rw_semaphore *sem)
+static inline int __down_read_trylock(struct compat_rw_semaphore *sem)
 {
 	int tmp;
 
@@ -98,7 +98,7 @@ static inline int __down_read_trylock(st
 /*
  * lock for writing
  */
-static inline void __down_write(struct rw_semaphore *sem)
+static inline void __down_write(struct compat_rw_semaphore *sem)
 {
 	int tmp;
 
@@ -110,7 +110,7 @@ static inline void __down_write(struct r
 		rwsem_down_write_failed(sem);
 }
 
-static inline int __down_write_trylock(struct rw_semaphore *sem)
+static inline int __down_write_trylock(struct compat_rw_semaphore *sem)
 {
 	int tmp;
 
@@ -123,7 +123,7 @@ static inline int __down_write_trylock(s
 /*
  * unlock after reading
  */
-static inline void __up_read(struct rw_semaphore *sem)
+static inline void __up_read(struct compat_rw_semaphore *sem)
 {
 	int tmp;
 
@@ -136,7 +136,7 @@ static inline void __up_read(struct rw_s
 /*
  * unlock after writing
  */
-static inline void __up_write(struct rw_semaphore *sem)
+static inline void __up_write(struct compat_rw_semaphore *sem)
 {
 	smp_wmb();
 	if (atomic_sub_return(RWSEM_ACTIVE_WRITE_BIAS,
@@ -147,7 +147,7 @@ static inline void __up_write(struct rw_
 /*
  * implement atomic add functionality
  */
-static inline void rwsem_atomic_add(int delta, struct rw_semaphore *sem)
+static inline void rwsem_atomic_add(int delta, struct compat_rw_semaphore *sem)
 {
 	atomic_add(delta, (atomic_t *)(&sem->count));
 }
@@ -155,7 +155,7 @@ static inline void rwsem_atomic_add(int 
 /*
  * downgrade write lock to read lock
  */
-static inline void __downgrade_write(struct rw_semaphore *sem)
+static inline void __downgrade_write(struct compat_rw_semaphore *sem)
 {
 	int tmp;
 
@@ -165,7 +165,7 @@ static inline void __downgrade_write(str
 		rwsem_downgrade_wake(sem);
 }
 
-static inline void __down_write_nested(struct rw_semaphore *sem, int subclass)
+static inline void __down_write_nested(struct compat_rw_semaphore *sem, int subclass)
 {
 	__down_write(sem);
 }
@@ -173,13 +173,13 @@ static inline void __down_write_nested(s
 /*
  * implement exchange and add functionality
  */
-static inline int rwsem_atomic_update(int delta, struct rw_semaphore *sem)
+static inline int rwsem_atomic_update(int delta, struct compat_rw_semaphore *sem)
 {
 	smp_mb();
 	return atomic_add_return(delta, (atomic_t *)(&sem->count));
 }
 
-static inline int rwsem_is_locked(struct rw_semaphore *sem)
+static inline int rwsem_is_locked(struct compat_rw_semaphore *sem)
 {
 	return (sem->count != 0);
 }
Index: linux-2.6.23.1-rt5/include/asm-sh/semaphore-helper.h
===================================================================
--- linux-2.6.23.1-rt5.orig/include/asm-sh/semaphore-helper.h
+++ linux-2.6.23.1-rt5/include/asm-sh/semaphore-helper.h
@@ -14,12 +14,12 @@
  * This is trivially done with load_locked/store_cond,
  * which we have.  Let the rest of the losers suck eggs.
  */
-static __inline__ void wake_one_more(struct semaphore * sem)
+static __inline__ void wake_one_more(struct compat_semaphore * sem)
 {
 	atomic_inc((atomic_t *)&sem->sleepers);
 }
 
-static __inline__ int waking_non_zero(struct semaphore *sem)
+static __inline__ int waking_non_zero(struct compat_semaphore *sem)
 {
 	unsigned long flags;
 	int ret = 0;
@@ -43,7 +43,7 @@ static __inline__ int waking_non_zero(st
  * protected by the spinlock in order to make atomic this atomic_inc() with the
  * atomic_read() in wake_one_more(), otherwise we can race. -arca
  */
-static __inline__ int waking_non_zero_interruptible(struct semaphore *sem,
+static __inline__ int waking_non_zero_interruptible(struct compat_semaphore *sem,
 						struct task_struct *tsk)
 {
 	unsigned long flags;
@@ -70,7 +70,7 @@ static __inline__ int waking_non_zero_in
  * protected by the spinlock in order to make atomic this atomic_inc() with the
  * atomic_read() in wake_one_more(), otherwise we can race. -arca
  */
-static __inline__ int waking_non_zero_trylock(struct semaphore *sem)
+static __inline__ int waking_non_zero_trylock(struct compat_semaphore *sem)
 {
 	unsigned long flags;
 	int ret = 1;
Index: linux-2.6.23.1-rt5/include/asm-sh/semaphore.h
===================================================================
--- linux-2.6.23.1-rt5.orig/include/asm-sh/semaphore.h
+++ linux-2.6.23.1-rt5/include/asm-sh/semaphore.h
@@ -20,29 +20,36 @@
 #include <asm/system.h>
 #include <asm/atomic.h>
 
-struct semaphore {
+/*
+ * On !PREEMPT_RT all semaphores are compat:
+ */
+#ifndef CONFIG_PREEMPT_RT
+# define compat_semaphore semaphore
+#endif
+
+struct compat_semaphore {
 	atomic_t count;
 	int sleepers;
 	wait_queue_head_t wait;
 };
 
-#define __SEMAPHORE_INITIALIZER(name, n)				\
+#define __COMPAT_SEMAPHORE_INITIALIZER(name, n)				\
 {									\
 	.count		= ATOMIC_INIT(n),				\
 	.sleepers	= 0,						\
 	.wait		= __WAIT_QUEUE_HEAD_INITIALIZER((name).wait)	\
 }
 
-#define __DECLARE_SEMAPHORE_GENERIC(name,count) \
-	struct semaphore name = __SEMAPHORE_INITIALIZER(name,count)
+#define __COMPAT_DECLARE_SEMAPHORE_GENERIC(name,count) \
+	struct compat_semaphore name = __COMPAT_SEMAPHORE_INITIALIZER(name,count)
 
-#define DECLARE_MUTEX(name) __DECLARE_SEMAPHORE_GENERIC(name,1)
-#define DECLARE_MUTEX_LOCKED(name) __DECLARE_SEMAPHORE_GENERIC(name,0)
+#define COMPAT_DECLARE_MUTEX(name) __COMPAT_DECLARE_SEMAPHORE_GENERIC(name,1)
+#define COMPAT_DECLARE_MUTEX_LOCKED(name) __COMPAT_DECLARE_SEMAPHORE_GENERIC(name,0)
 
-static inline void sema_init (struct semaphore *sem, int val)
+static inline void compat_sema_init (struct compat_semaphore *sem, int val)
 {
 /*
- *	*sem = (struct semaphore)__SEMAPHORE_INITIALIZER((*sem),val);
+ *	*sem = (struct compat_semaphore)__SEMAPHORE_INITIALIZER((*sem),val);
  *
  * i'd rather use the more flexible initialization above, but sadly
  * GCC 2.7.2.3 emits a bogus warning. EGCS doesn't. Oh well.
@@ -52,14 +59,14 @@ static inline void sema_init (struct sem
 	init_waitqueue_head(&sem->wait);
 }
 
-static inline void init_MUTEX (struct semaphore *sem)
+static inline void compat_init_MUTEX (struct compat_semaphore *sem)
 {
-	sema_init(sem, 1);
+	compat_sema_init(sem, 1);
 }
 
-static inline void init_MUTEX_LOCKED (struct semaphore *sem)
+static inline void compat_init_MUTEX_LOCKED (struct compat_semaphore *sem)
 {
-	sema_init(sem, 0);
+	compat_sema_init(sem, 0);
 }
 
 #if 0
@@ -69,36 +76,36 @@ asmlinkage int  __down_failed_trylock(vo
 asmlinkage void __up_wakeup(void /* special register calling convention */);
 #endif
 
-asmlinkage void __down(struct semaphore * sem);
-asmlinkage int  __down_interruptible(struct semaphore * sem);
-asmlinkage int  __down_trylock(struct semaphore * sem);
-asmlinkage void __up(struct semaphore * sem);
+asmlinkage void __compat_down(struct compat_semaphore * sem);
+asmlinkage int  __compat_down_interruptible(struct compat_semaphore * sem);
+asmlinkage int  __compat_down_trylock(struct compat_semaphore * sem);
+asmlinkage void __compat_up(struct compat_semaphore * sem);
 
 extern spinlock_t semaphore_wake_lock;
 
-static inline void down(struct semaphore * sem)
+static inline void compat_down(struct compat_semaphore * sem)
 {
 	might_sleep();
 	if (atomic_dec_return(&sem->count) < 0)
-		__down(sem);
+		__compat_down(sem);
 }
 
-static inline int down_interruptible(struct semaphore * sem)
+static inline int compat_down_interruptible(struct compat_semaphore * sem)
 {
 	int ret = 0;
 
 	might_sleep();
 	if (atomic_dec_return(&sem->count) < 0)
-		ret = __down_interruptible(sem);
+		ret = __compat_down_interruptible(sem);
 	return ret;
 }
 
-static inline int down_trylock(struct semaphore * sem)
+static inline int compat_down_trylock(struct compat_semaphore * sem)
 {
 	int ret = 0;
 
 	if (atomic_dec_return(&sem->count) < 0)
-		ret = __down_trylock(sem);
+		ret = __compat_down_trylock(sem);
 	return ret;
 }
 
@@ -106,11 +113,17 @@ static inline int down_trylock(struct se
  * Note! This is subtle. We jump to wake people up only if
  * the semaphore was negative (== somebody was waiting on it).
  */
-static inline void up(struct semaphore * sem)
+static inline void compat_up(struct compat_semaphore * sem)
 {
 	if (atomic_inc_return(&sem->count) <= 0)
-		__up(sem);
+		__compat_up(sem);
 }
 
+extern int compat_sem_is_locked(struct compat_semaphore *sem);
+
+#define compat_sema_count(sem) atomic_read(&(sem)->count)
+
+#include <linux/semaphore.h>
+
 #endif
 #endif /* __ASM_SH_SEMAPHORE_H */
Index: linux-2.6.23.1-rt5/include/asm-sh/system.h
===================================================================
--- linux-2.6.23.1-rt5.orig/include/asm-sh/system.h
+++ linux-2.6.23.1-rt5/include/asm-sh/system.h
@@ -158,10 +158,10 @@ static inline unsigned long xchg_u32(vol
 {
 	unsigned long flags, retval;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	retval = *m;
 	*m = val;
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 	return retval;
 }
 
@@ -169,10 +169,10 @@ static inline unsigned long xchg_u8(vola
 {
 	unsigned long flags, retval;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	retval = *m;
 	*m = val & 0xff;
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 	return retval;
 }
 
@@ -207,11 +207,11 @@ static inline unsigned long __cmpxchg_u3
 	__u32 retval;
 	unsigned long flags;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 	retval = *m;
 	if (retval == old)
 		*m = new;
-	local_irq_restore(flags);       /* implies memory barrier  */
+	raw_local_irq_restore(flags);       /* implies memory barrier  */
 	return retval;
 }
 
Index: linux-2.6.23.1-rt5/include/asm-sh/thread_info.h
===================================================================
--- linux-2.6.23.1-rt5.orig/include/asm-sh/thread_info.h
+++ linux-2.6.23.1-rt5/include/asm-sh/thread_info.h
@@ -111,6 +111,7 @@ static inline struct thread_info *curren
 #define TIF_NEED_RESCHED	2	/* rescheduling necessary */
 #define TIF_RESTORE_SIGMASK	3	/* restore signal mask in do_signal() */
 #define TIF_SINGLESTEP		4	/* singlestepping active */
+#define TIF_NEED_RESCHED_DELAYED	6	/* reschedule on return to userspace */
 #define TIF_USEDFPU		16	/* FPU was used by this task this quantum (SMP) */
 #define TIF_POLLING_NRFLAG	17	/* true if poll_idle() is polling TIF_NEED_RESCHED */
 #define TIF_MEMDIE		18
@@ -121,6 +122,7 @@ static inline struct thread_info *curren
 #define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
 #define _TIF_RESTORE_SIGMASK	(1<<TIF_RESTORE_SIGMASK)
 #define _TIF_SINGLESTEP		(1<<TIF_SINGLESTEP)
+#define _TIF_NEED_RESCHED_DELAYED	(1<<TIF_NEED_RESCHED_DELAYED)
 #define _TIF_USEDFPU		(1<<TIF_USEDFPU)
 #define _TIF_POLLING_NRFLAG	(1<<TIF_POLLING_NRFLAG)
 #define _TIF_FREEZE		(1<<TIF_FREEZE)
