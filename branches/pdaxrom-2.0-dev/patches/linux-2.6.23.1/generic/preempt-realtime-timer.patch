---
 include/linux/hrtimer.h      |    2 -
 include/linux/time.h         |    2 -
 kernel/time/clockevents.c    |    2 -
 kernel/time/clocksource.c    |    2 -
 kernel/time/tick-broadcast.c |    2 -
 kernel/time/tick-common.c    |    2 -
 kernel/time/tick-internal.h  |    2 -
 kernel/time/tick-sched.c     |    2 -
 kernel/time/timekeeping.c    |    2 -
 kernel/time/timer_stats.c    |    6 ++---
 kernel/timer.c               |   48 ++++++++++++++++++++++++++++++++++++++++---
 11 files changed, 57 insertions(+), 15 deletions(-)

Index: linux-2.6.23.1-rt5/include/linux/hrtimer.h
===================================================================
--- linux-2.6.23.1-rt5.orig/include/linux/hrtimer.h
+++ linux-2.6.23.1-rt5/include/linux/hrtimer.h
@@ -191,7 +191,7 @@ struct hrtimer_clock_base {
  * @nr_events:		Total number of timer interrupt events
  */
 struct hrtimer_cpu_base {
-	spinlock_t			lock;
+	raw_spinlock_t			lock;
 	struct lock_class_key		lock_key;
 	struct hrtimer_clock_base	clock_base[HRTIMER_MAX_CLOCK_BASES];
 #ifdef CONFIG_HIGH_RES_TIMERS
Index: linux-2.6.23.1-rt5/include/linux/time.h
===================================================================
--- linux-2.6.23.1-rt5.orig/include/linux/time.h
+++ linux-2.6.23.1-rt5/include/linux/time.h
@@ -92,7 +92,7 @@ static inline struct timespec timespec_s
 
 extern struct timespec xtime;
 extern struct timespec wall_to_monotonic;
-extern seqlock_t xtime_lock __attribute__((weak));
+extern raw_seqlock_t xtime_lock __attribute__((weak));
 
 extern unsigned long read_persistent_clock(void);
 extern int update_persistent_clock(struct timespec now);
Index: linux-2.6.23.1-rt5/kernel/time/clockevents.c
===================================================================
--- linux-2.6.23.1-rt5.orig/kernel/time/clockevents.c
+++ linux-2.6.23.1-rt5/kernel/time/clockevents.c
@@ -29,7 +29,7 @@ static LIST_HEAD(clockevents_released);
 static RAW_NOTIFIER_HEAD(clockevents_chain);
 
 /* Protection for the above */
-static DEFINE_SPINLOCK(clockevents_lock);
+static DEFINE_RAW_SPINLOCK(clockevents_lock);
 
 /**
  * clockevents_delta2ns - Convert a latch value (device ticks) to nanoseconds
Index: linux-2.6.23.1-rt5/kernel/time/clocksource.c
===================================================================
--- linux-2.6.23.1-rt5.orig/kernel/time/clocksource.c
+++ linux-2.6.23.1-rt5/kernel/time/clocksource.c
@@ -51,7 +51,7 @@ static struct clocksource *curr_clocksou
 static struct clocksource *next_clocksource;
 static struct clocksource *clocksource_override;
 static LIST_HEAD(clocksource_list);
-static DEFINE_SPINLOCK(clocksource_lock);
+static DEFINE_RAW_SPINLOCK(clocksource_lock);
 static char override_name[32];
 static int finished_booting;
 
Index: linux-2.6.23.1-rt5/kernel/time/tick-broadcast.c
===================================================================
--- linux-2.6.23.1-rt5.orig/kernel/time/tick-broadcast.c
+++ linux-2.6.23.1-rt5/kernel/time/tick-broadcast.c
@@ -29,7 +29,7 @@
 
 struct tick_device tick_broadcast_device;
 static cpumask_t tick_broadcast_mask;
-static DEFINE_SPINLOCK(tick_broadcast_lock);
+static DEFINE_RAW_SPINLOCK(tick_broadcast_lock);
 
 #ifdef CONFIG_TICK_ONESHOT
 static void tick_broadcast_clear_oneshot(int cpu);
Index: linux-2.6.23.1-rt5/kernel/time/tick-common.c
===================================================================
--- linux-2.6.23.1-rt5.orig/kernel/time/tick-common.c
+++ linux-2.6.23.1-rt5/kernel/time/tick-common.c
@@ -32,7 +32,7 @@ DEFINE_PER_CPU(struct tick_device, tick_
 ktime_t tick_next_period;
 ktime_t tick_period;
 int tick_do_timer_cpu __read_mostly = -1;
-DEFINE_SPINLOCK(tick_device_lock);
+DEFINE_RAW_SPINLOCK(tick_device_lock);
 
 /*
  * Debugging: see timer_list.c
Index: linux-2.6.23.1-rt5/kernel/time/tick-internal.h
===================================================================
--- linux-2.6.23.1-rt5.orig/kernel/time/tick-internal.h
+++ linux-2.6.23.1-rt5/kernel/time/tick-internal.h
@@ -2,7 +2,7 @@
  * tick internal variable and functions used by low/high res code
  */
 DECLARE_PER_CPU(struct tick_device, tick_cpu_device);
-extern spinlock_t tick_device_lock;
+extern raw_spinlock_t tick_device_lock;
 extern ktime_t tick_next_period;
 extern ktime_t tick_period;
 extern int tick_do_timer_cpu __read_mostly;
Index: linux-2.6.23.1-rt5/kernel/time/tick-sched.c
===================================================================
--- linux-2.6.23.1-rt5.orig/kernel/time/tick-sched.c
+++ linux-2.6.23.1-rt5/kernel/time/tick-sched.c
@@ -176,7 +176,7 @@ void tick_nohz_stop_sched_tick(void)
 	if (unlikely(ts->nohz_mode == NOHZ_MODE_INACTIVE))
 		goto end;
 
-	if (need_resched())
+	if (need_resched() || need_resched_delayed())
 		goto end;
 
 	cpu = smp_processor_id();
Index: linux-2.6.23.1-rt5/kernel/time/timekeeping.c
===================================================================
--- linux-2.6.23.1-rt5.orig/kernel/time/timekeeping.c
+++ linux-2.6.23.1-rt5/kernel/time/timekeeping.c
@@ -24,7 +24,7 @@
  * This read-write spinlock protects us from races in SMP while
  * playing with xtime and avenrun.
  */
-__attribute__((weak)) __cacheline_aligned_in_smp DEFINE_SEQLOCK(xtime_lock);
+__attribute__((weak)) __cacheline_aligned_in_smp DEFINE_RAW_SEQLOCK(xtime_lock);
 
 EXPORT_SYMBOL(xtime_lock);
 
Index: linux-2.6.23.1-rt5/kernel/time/timer_stats.c
===================================================================
--- linux-2.6.23.1-rt5.orig/kernel/time/timer_stats.c
+++ linux-2.6.23.1-rt5/kernel/time/timer_stats.c
@@ -81,12 +81,12 @@ struct entry {
 /*
  * Spinlock protecting the tables - not taken during lookup:
  */
-static DEFINE_SPINLOCK(table_lock);
+static DEFINE_RAW_SPINLOCK(table_lock);
 
 /*
  * Per-CPU lookup locks for fast hash lookup:
  */
-static DEFINE_PER_CPU(spinlock_t, lookup_lock);
+static DEFINE_PER_CPU(raw_spinlock_t, lookup_lock);
 
 /*
  * Mutex to serialize state changes with show-stats activities:
@@ -238,7 +238,7 @@ void timer_stats_update_stats(void *time
 	/*
 	 * It doesnt matter which lock we take:
 	 */
-	spinlock_t *lock;
+	raw_spinlock_t *lock = &per_cpu(lookup_lock, raw_smp_processor_id());
 	struct entry *entry, input;
 	unsigned long flags;
 
Index: linux-2.6.23.1-rt5/kernel/timer.c
===================================================================
--- linux-2.6.23.1-rt5.orig/kernel/timer.c
+++ linux-2.6.23.1-rt5/kernel/timer.c
@@ -850,9 +850,22 @@ unsigned long get_next_timer_interrupt(u
 	tvec_base_t *base = __get_cpu_var(tvec_bases);
 	unsigned long expires;
 
+#ifdef CONFIG_PREEMPT_RT
+	/*
+	 * On PREEMPT_RT we cannot sleep here. If the trylock does not
+	 * succeed then we return the worst-case 'expires in 1 tick'
+	 * value:
+	 */
+	if (spin_trylock(&base->lock)) {
+		expires = __next_timer_interrupt(base);
+		spin_unlock(&base->lock);
+	} else
+		expires = now + 1;
+#else
 	spin_lock(&base->lock);
 	expires = __next_timer_interrupt(base);
 	spin_unlock(&base->lock);
+#endif
 
 	if (time_before_eq(expires, now))
 		return now;
@@ -875,8 +888,8 @@ unsigned long next_timer_interrupt(void)
  */
 void update_process_times(int user_tick)
 {
-	struct task_struct *p = current;
 	int cpu = smp_processor_id();
+	struct task_struct *p = current;
 
 	/* Note: this timer irq context must be accounted for as well. */
 	if (user_tick)
@@ -895,8 +908,29 @@ void update_process_times(int user_tick)
  */
 static unsigned long count_active_tasks(void)
 {
+	/*
+	 * On PREEMPT_RT, we are running in the timer softirq thread,
+	 * so consider 1 less running tasks:
+	 */
+#ifdef CONFIG_PREEMPT_RT
+	return (nr_active() - 1) * FIXED_1;
+#else
 	return nr_active() * FIXED_1;
+#endif
+}
+
+#ifdef CONFIG_PREEMPT_RT
+/*
+ * Nr of active tasks - counted in fixed-point numbers
+ */
+static unsigned long count_active_rt_tasks(void)
+{
+	extern unsigned long rt_nr_running(void);
+	extern unsigned long rt_nr_uninterruptible(void);
+
+	return (rt_nr_running() + rt_nr_uninterruptible()) * FIXED_1;
 }
+#endif
 
 /*
  * Hmm.. Changed this, as the GNU make sources (load.c) seems to
@@ -910,6 +944,8 @@ unsigned long avenrun[3];
 
 EXPORT_SYMBOL(avenrun);
 
+unsigned long avenrun_rt[3];
+
 /*
  * calc_load - given tick count, update the avenrun load estimates.
  * This is called while holding a write_lock on xtime_lock.
@@ -928,6 +964,12 @@ static inline void calc_load(unsigned lo
 			CALC_LOAD(avenrun[2], EXP_15, active_tasks);
 			count += LOAD_FREQ;
 		} while (count < 0);
+#ifdef CONFIG_PREEMPT_RT
+		active_tasks = count_active_rt_tasks();
+		CALC_LOAD(avenrun_rt[0], EXP_1, active_tasks);
+		CALC_LOAD(avenrun_rt[1], EXP_5, active_tasks);
+		CALC_LOAD(avenrun_rt[2], EXP_15, active_tasks);
+#endif
 	}
 }
 
@@ -1351,7 +1393,7 @@ static void __devinit migrate_timers(int
 	old_base = per_cpu(tvec_bases, cpu);
 	new_base = get_cpu_var(tvec_bases);
 
-	local_irq_disable();
+	local_irq_disable_nort();
 	double_spin_lock(&new_base->lock, &old_base->lock,
 			 smp_processor_id() < cpu);
 
@@ -1368,7 +1410,7 @@ static void __devinit migrate_timers(int
 
 	double_spin_unlock(&new_base->lock, &old_base->lock,
 			   smp_processor_id() < cpu);
-	local_irq_enable();
+	local_irq_enable_nort();
 	put_cpu_var(tvec_bases);
 }
 #endif /* CONFIG_HOTPLUG_CPU */
